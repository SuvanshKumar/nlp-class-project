
Phase A - tweet annotation (submit by April 16)

200 annotations per student (1000 per team), (TA will later (on April 18) give 1000 more tweets, so we will have 2000 annotated tweets to work with)

How to annotate?
	First read the misinformation target and then read the tweet itself. Then make a judgement on whether the tweet agrees or disagrees with the misinformation target

Our annotations will be matched against the same tweets annotated by the TA and percentage of matching stances will give us a score (all tweets matching -> 100%, 80% matching -> 80%, etc.)

Note: make sure the annotations are formatted correctly (the jsonl format, so they are readable by the program). The format is straightforward jsonl, and TA will give (or has given) examples that show the real tweets in those formats.

Total points: 20

If our score is above a threshold of 70%, we will get full 20 points, otherwise zero points (according to TA, but professor says we might get a partial credit even if below the threshold).
(Unclear on whether this may change, and whether 100% will still be needed to get 100% marks)

Phase B - Building stance detection systems using various neural architectures

Data set of 2000 tweets, we shuffle and split among the training set and testing validation set, using which we train and evaluate our systems. 

(According to my understanding, each student will have access to the same 2000 tweets, but we may independently divide them into training/validation sets randomly, and these divisions need not be same for all the students) -- wrong

--correction--
We need the SAME training and validation splitting FOR THE WHOLE TEAM.

Phase C - Evaluating stance detection systems on test data (April 29 to May 1)

Each team picks one of the 4 (or 5?) systems that we have made (based on which gives better accuracy / recall / precision / faster, etc. (most likely accuracy only)).

On April 29, TA will release the test data (without the labels). We will have the chance to run our models on that data before we choose which model to pick for leaderboard. We have to make predictions on the test data and submit those predictions (on elearning, by 1st May). The TA will compare these predictions to the real labels of the testing set (which will be unknown to us), and give us scores. Then team leaderboard will be displayed on elearning showing the scores of the teams.

Final demos (week of May 3rd)

One single report per team, needs to be sent to the TA before the presentation.
